{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import equinox as eqx\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "from jaxtyping import Array, Float, PRNGKeyArray, PyTree\n",
    "from icecream import ic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_multiple_of_64(number: int) -> int:\n",
    "    while number % 64 != 0:\n",
    "        number += 1\n",
    "    return number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded=[31373, 11, 995]\n",
      "n_vocab=50304\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "encoded = enc.encode(\"hello, world\")\n",
    "print(f\"{encoded=}\")\n",
    "# pad enc.n_vocab to nearest multiple of 64 to make it even and for efficiency\n",
    "n_vocab = get_next_multiple_of_64(enc.n_vocab)\n",
    "print(f\"{n_vocab=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoded='hello, world'\n"
     ]
    }
   ],
   "source": [
    "decoded = enc.decode(encoded)\n",
    "print(f\"{decoded=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_positional_encoding(n_tokens: int, n_vocab: int) -> Float[Array, \"n_tokens n_vocab\"]:     \n",
    "    pos = jnp.arange(n_tokens)[:, jnp.newaxis]\n",
    "    div_term = jnp.exp(jnp.arange(0, n_vocab, 2) * -(jnp.log(10000.0) / n_vocab))\n",
    "    # alternatively: div_term = 1 / 10000 ** (jnp.arange(0, D, 2) / D) \n",
    "    # that's closer to the actual notation they used. \n",
    "    pos_enc = jnp.zeros((n_tokens, n_vocab))\n",
    "    pos_enc = pos_enc.at[:, 0::2].set(jnp.sin(pos * div_term))\n",
    "    pos_enc = pos_enc.at[:, 1::2].set(jnp.cos(pos * div_term))\n",
    "    return pos_enc \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| attention_scores.shape: (4, 2, 4)\n",
      "ic| mask.shape: (4, 1, 4)\n",
      "ic| attention_scores.shape: (4, 2, 4)\n",
      "ic| output.shape: (4, 128)\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttention(eqx.Module):\n",
    "    n_heads: int = eqx.field(static=True)\n",
    "    qkv_size: int = eqx.field(static=True)\n",
    "\n",
    "    query: eqx.nn.Linear\n",
    "    key: eqx.nn.Linear\n",
    "    value: eqx.nn.Linear\n",
    "\n",
    "    output: eqx.nn.Linear\n",
    "    def __init__(self, input_dim: int, n_heads: int, key: PRNGKeyArray) -> None:\n",
    "        key, *subkeys = jax.random.split(key, 5)\n",
    "\n",
    "        self.qkv_size = input_dim // n_heads\n",
    "        \n",
    "        self.query = eqx.nn.Linear(in_features=input_dim, out_features=n_heads * self.qkv_size, key=subkeys[0], use_bias=False)\n",
    "        self.key = eqx.nn.Linear(in_features=input_dim, out_features=n_heads * self.qkv_size, key=subkeys[1], use_bias=False)\n",
    "        self.value = eqx.nn.Linear(in_features=input_dim, out_features=n_heads * self.qkv_size, key=subkeys[2], use_bias=False)\n",
    "\n",
    "        self.output = eqx.nn.Linear(in_features=input_dim, out_features=input_dim, key=subkeys[3], use_bias=False) \n",
    "\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "    def _project(self, proj, x):\n",
    "        seq_length, _ = x.shape\n",
    "        projection = jax.vmap(proj)(x)\n",
    "        return projection.reshape(seq_length, self.n_heads, -1)\n",
    "\n",
    "    def __call__(self, x: Array, masking: bool):\n",
    "        T, _ = x.shape\n",
    "\n",
    "        q = self._project(self.query, x)\n",
    "        k = self._project(self.key, x)\n",
    "        v = self._project(self.value, x)\n",
    "\n",
    "        assert q.shape == (T, self.n_heads, self.qkv_size)\n",
    "        assert k.shape == (T, self.n_heads, self.qkv_size)\n",
    "        assert v.shape == (T, self.n_heads, self.qkv_size)\n",
    "\n",
    "        dot_product_vmap = jax.vmap(\n",
    "            lambda q, k: jnp.dot(q, k.T), \n",
    "            in_axes=(1, 1), \n",
    "            out_axes=1\n",
    "        )\n",
    "        attention_scores = dot_product_vmap(q, k)\n",
    "        ic(attention_scores.shape)\n",
    "        attention_scores = attention_scores / jnp.sqrt(self.qkv_size)\n",
    "        if masking:\n",
    "            mask = jnp.tril(jnp.ones(shape=(T, T))) == 1\n",
    "            mask = jnp.expand_dims(mask, axis=1) # we add an extra dimension at axis 1 for broadcasting\n",
    "            ic(mask.shape)\n",
    "            attention_scores = jnp.where(mask, attention_scores, float(\"-inf\"))\n",
    "            # print(f\"{attention_scores}\")\n",
    "        \n",
    "        attention_scores = jax.nn.softmax(attention_scores, axis=-1)\n",
    "        ic(attention_scores.shape)\n",
    "        matmul_vmap = jax.vmap(\n",
    "            lambda s, v: jnp.dot(s, v), \n",
    "            in_axes=(1, 1), \n",
    "            out_axes=1\n",
    "        )\n",
    "\n",
    "        output = matmul_vmap(attention_scores, v)\n",
    "        # print(f\"before reshaping {output.shape=}\")\n",
    "        output = output.reshape(T, -1)\n",
    "        # print(f\"after reshaping {output.shape=}\")\n",
    "        output = jax.vmap(self.output)(output)\n",
    "        ic(output.shape)\n",
    "        return output\n",
    "    \n",
    "n_vocab = 128\n",
    "N_HEADS = 2\n",
    "N_EMBD = 4096\n",
    "T = 4 # 4 tokens \n",
    "mha = MultiHeadAttention(\n",
    "    input_dim=n_vocab,\n",
    "    n_heads=N_HEADS,\n",
    "    key=jax.random.PRNGKey(21)\n",
    ")\n",
    "\n",
    "x = jax.random.uniform(shape=(T, n_vocab), key=jax.random.PRNGKey(11))\n",
    "output = mha(x, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0.]\n",
      " [1. 1. 0. 0.]\n",
      " [1. 1. 1. 0.]\n",
      " [1. 1. 1. 1.]]\n",
      "[[  0. -inf -inf -inf]\n",
      " [  0.   0. -inf -inf]\n",
      " [  0.   0.   0. -inf]\n",
      " [  0.   0.   0.   0.]]\n",
      "[[1.         0.         0.         0.        ]\n",
      " [0.5        0.5        0.         0.        ]\n",
      " [0.33333334 0.33333334 0.33333334 0.        ]\n",
      " [0.25       0.25       0.25       0.25      ]]\n"
     ]
    }
   ],
   "source": [
    "T = 4\n",
    "tril = jnp.tril(jnp.ones(shape=(T, T)))\n",
    "print(tril)\n",
    "mask = jnp.where(tril == 0, jnp.full(shape=(T, T), fill_value=float(\"-inf\")), jnp.zeros(shape=(T,T)))\n",
    "print(f\"{mask}\")\n",
    "mask = jax.nn.softmax(mask, axis=-1)\n",
    "print(f\"{mask}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask.shape=(4, 1, 4)\n",
      "logits.shape=(4, 2, 4)\n"
     ]
    }
   ],
   "source": [
    "T = 4\n",
    "h = 2\n",
    "mask = jnp.tril(jnp.ones(shape=(T, T))) == 1\n",
    "mask = jnp.expand_dims(mask, axis=1)\n",
    "print(f\"{mask.shape=}\")\n",
    "logits = jax.random.uniform(shape=(T, h, T), key=jax.random.PRNGKey(0))\n",
    "logits = jnp.where(mask, logits, float(\"-inf\"))\n",
    "logits = jax.nn.softmax(logits, axis=-1)\n",
    "print(f\"{logits.shape=}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_indices.shape=(4, 1)\n",
      "[[ True False False False]\n",
      " [ True  True False False]\n",
      " [ True  True  True False]\n",
      " [ True  True  True  True]]\n",
      "jnp.finfo(logits.dtype).min=-3.4028235e+38\n",
      "[[0.49120486 0.09953129 0.8435687  0.6532923 ]\n",
      " [0.7960056  0.16815436 0.27717125 0.25922954]\n",
      " [0.77414536 0.59465444 0.42191577 0.20185745]\n",
      " [0.11166441 0.01811409 0.218642   0.5060872 ]]\n",
      "[[ 4.91204858e-01 -3.40282347e+38 -3.40282347e+38 -3.40282347e+38]\n",
      " [ 7.96005607e-01  1.68154359e-01 -3.40282347e+38 -3.40282347e+38]\n",
      " [ 7.74145365e-01  5.94654441e-01  4.21915770e-01 -3.40282347e+38]\n",
      " [ 1.11664414e-01  1.81140900e-02  2.18641996e-01  5.06087184e-01]]\n",
      "[[1.         0.         0.         0.        ]\n",
      " [0.6520021  0.3479979  0.         0.        ]\n",
      " [0.3938847  0.32916766 0.27694768 0.        ]\n",
      " [0.22187074 0.20205593 0.246922   0.32915136]]\n"
     ]
    }
   ],
   "source": [
    "T = 4\n",
    "causal_mask_offset = 0\n",
    "query_indices = jnp.arange(T)[:, None]\n",
    "print(f\"{query_indices.shape=}\")\n",
    "kv_indices = jnp.arange(T)[None, :]\n",
    "mask = kv_indices <= query_indices + causal_mask_offset\n",
    "print(mask)\n",
    "logits = jax.random.uniform(shape=(mask.shape), key=jax.random.PRNGKey(221))\n",
    "print(f\"{jnp.finfo(logits.dtype).min=}\")\n",
    "print(logits)\n",
    "logits = jnp.where(mask, logits, jnp.finfo(logits.dtype).min) # for more numerical stability\n",
    "print(logits)\n",
    "logits = jax.nn.softmax(logits, axis=-1)\n",
    "print(logits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(eqx.Module):\n",
    "    weight: Array\n",
    "    eps: float\n",
    "\n",
    "    def __init__(self, dim: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = jnp.ones(dim)\n",
    "\n",
    "    def _norm(self, x: Array):\n",
    "        return x * jax.lax.rsqrt(jnp.mean(x**2, axis=-1, keepdims=True) + self.eps)\n",
    "\n",
    "    def __call__(self, x: Array) -> Array:\n",
    "        output = self._norm(x)\n",
    "        return output * self.weight\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| attention_scores.shape: (8, 4, 8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "side effect\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| mask.shape: (8, 1, 8)\n",
      "ic| attention_scores.shape: (8, 4, 8)\n",
      "ic| output.shape: (8, 4096)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(8, 128)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Transformer(eqx.Module):\n",
    "    input_embedding: eqx.nn.Embedding\n",
    "    masked_mha: MultiHeadAttention\n",
    "    feedforward: eqx.nn.MLP\n",
    "    rms_norm: RMSNorm\n",
    "\n",
    "    output: eqx.nn.Linear\n",
    "    positional_encoding: Array \n",
    "\n",
    "    def __init__(self, n_dims: int, n_embd: int, n_heads: int, key: PRNGKeyArray, width_size: int=32, depth: int = 2, max_token_size: int = 8) -> None:\n",
    "        key, *subkeys = jax.random.split(key, 20) # let's just split 20 for now, we'll probably need them later\n",
    "        self.input_embedding = eqx.nn.Embedding(n_dims, n_embd, key=subkeys[0])\n",
    "        self.masked_mha = MultiHeadAttention(input_dim=n_embd, n_heads=n_heads, key=subkeys[1])\n",
    "\n",
    "        # Equinox has a built-in MLP module\n",
    "        self.feedforward = eqx.nn.MLP(in_size=n_embd, out_size=n_embd, width_size=width_size, key=subkeys[2], depth=depth)\n",
    "        self.positional_encoding = get_positional_encoding(max_token_size, n_embd)\n",
    "\n",
    "        self.rms_norm = RMSNorm(dim=n_embd)\n",
    "\n",
    "        self.output = eqx.nn.Linear(in_features=n_embd, out_features=n_dims, key=subkeys[4], use_bias=False)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        print(f\"side effect\")\n",
    "        x = jax.vmap(self.input_embedding)(x)\n",
    "        x += self.positional_encoding\n",
    "        x = self.rms_norm(self.masked_mha(x, masking=True) + x) # residual connection\n",
    "        x = self.rms_norm(jax.vmap(self.feedforward)(x) + x) # residual connection\n",
    "        x = jax.vmap(self.output)(x)\n",
    "        # x = jax.nn.softmax(x) # we don't softmax here, because we want the raw logits for our loss function \n",
    "        # but you can totally softmax here and inverse that later; \n",
    "        return x \n",
    "\n",
    "\n",
    "key = jax.random.PRNGKey(42)\n",
    "INPUT_DIMS = 128\n",
    "N_EMBD = 4096\n",
    "N_HEADS = 4\n",
    "MAX_T = 8\n",
    "transformer = Transformer(n_dims=INPUT_DIMS, n_embd=N_EMBD, n_heads=N_HEADS, key=key)\n",
    "\n",
    "x = jnp.ones(shape=(MAX_T), dtype=jnp.int32)\n",
    "\n",
    "transformer(x).shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "side effect\n",
      "side effect\n",
      "0. loss=Array(4.3851104, dtype=float32), eval_loss=Array(4.289815, dtype=float32)\n",
      "100. loss=Array(3.234735, dtype=float32), eval_loss=Array(3.4245338, dtype=float32)\n",
      "200. loss=Array(3.30385, dtype=float32), eval_loss=Array(3.2770212, dtype=float32)\n",
      "300. loss=Array(2.545452, dtype=float32), eval_loss=Array(3.1839383, dtype=float32)\n",
      "400. loss=Array(2.1868153, dtype=float32), eval_loss=Array(3.1490939, dtype=float32)\n",
      "500. loss=Array(2.4573233, dtype=float32), eval_loss=Array(3.0842142, dtype=float32)\n",
      "600. loss=Array(3.0487125, dtype=float32), eval_loss=Array(3.0219848, dtype=float32)\n",
      "700. loss=Array(2.1939778, dtype=float32), eval_loss=Array(2.9301224, dtype=float32)\n",
      "800. loss=Array(1.9340211, dtype=float32), eval_loss=Array(2.9110885, dtype=float32)\n",
      "900. loss=Array(2.3659914, dtype=float32), eval_loss=Array(2.970211, dtype=float32)\n",
      "1000. loss=Array(2.9821432, dtype=float32), eval_loss=Array(2.8746958, dtype=float32)\n",
      "1100. loss=Array(2.7669654, dtype=float32), eval_loss=Array(2.8809278, dtype=float32)\n",
      "1200. loss=Array(2.863174, dtype=float32), eval_loss=Array(2.867341, dtype=float32)\n",
      "1300. loss=Array(1.7138388, dtype=float32), eval_loss=Array(2.8305702, dtype=float32)\n",
      "1400. loss=Array(2.8323483, dtype=float32), eval_loss=Array(2.8400168, dtype=float32)\n",
      "1500. loss=Array(1.8609931, dtype=float32), eval_loss=Array(2.8281384, dtype=float32)\n",
      "1600. loss=Array(1.6840352, dtype=float32), eval_loss=Array(2.8023689, dtype=float32)\n",
      "1700. loss=Array(2.450756, dtype=float32), eval_loss=Array(2.8144734, dtype=float32)\n",
      "1800. loss=Array(3.1667643, dtype=float32), eval_loss=Array(2.8518384, dtype=float32)\n",
      "1900. loss=Array(2.2452762, dtype=float32), eval_loss=Array(2.7800858, dtype=float32)\n",
      "2000. loss=Array(3.15266, dtype=float32), eval_loss=Array(2.7906919, dtype=float32)\n",
      "2100. loss=Array(1.5365113, dtype=float32), eval_loss=Array(2.7724316, dtype=float32)\n",
      "2200. loss=Array(2.478143, dtype=float32), eval_loss=Array(2.7781851, dtype=float32)\n",
      "2300. loss=Array(2.5879278, dtype=float32), eval_loss=Array(2.804898, dtype=float32)\n",
      "2400. loss=Array(2.3657768, dtype=float32), eval_loss=Array(2.764054, dtype=float32)\n",
      "2500. loss=Array(2.326349, dtype=float32), eval_loss=Array(2.7584531, dtype=float32)\n",
      "2600. loss=Array(2.6766152, dtype=float32), eval_loss=Array(2.7448475, dtype=float32)\n",
      "2700. loss=Array(2.957616, dtype=float32), eval_loss=Array(2.7272463, dtype=float32)\n",
      "2800. loss=Array(1.0326145, dtype=float32), eval_loss=Array(2.707023, dtype=float32)\n",
      "2900. loss=Array(2.7442636, dtype=float32), eval_loss=Array(2.7146218, dtype=float32)\n",
      "3000. loss=Array(2.5541427, dtype=float32), eval_loss=Array(2.684543, dtype=float32)\n",
      "3100. loss=Array(3.2601776, dtype=float32), eval_loss=Array(2.6669686, dtype=float32)\n",
      "3200. loss=Array(2.9777188, dtype=float32), eval_loss=Array(2.704236, dtype=float32)\n",
      "3300. loss=Array(2.5552402, dtype=float32), eval_loss=Array(2.675419, dtype=float32)\n",
      "3400. loss=Array(1.7224545, dtype=float32), eval_loss=Array(2.6971767, dtype=float32)\n",
      "3500. loss=Array(1.7329894, dtype=float32), eval_loss=Array(2.6816804, dtype=float32)\n",
      "3600. loss=Array(3.0344646, dtype=float32), eval_loss=Array(2.669998, dtype=float32)\n",
      "3700. loss=Array(2.007145, dtype=float32), eval_loss=Array(2.7015493, dtype=float32)\n",
      "3800. loss=Array(2.3619876, dtype=float32), eval_loss=Array(2.7502258, dtype=float32)\n",
      "3900. loss=Array(1.6461729, dtype=float32), eval_loss=Array(2.6597903, dtype=float32)\n",
      "4000. loss=Array(2.2674613, dtype=float32), eval_loss=Array(2.6259582, dtype=float32)\n",
      "4100. loss=Array(3.2570982, dtype=float32), eval_loss=Array(2.6308331, dtype=float32)\n",
      "4200. loss=Array(1.843973, dtype=float32), eval_loss=Array(2.6524634, dtype=float32)\n",
      "4300. loss=Array(3.014799, dtype=float32), eval_loss=Array(2.636153, dtype=float32)\n",
      "4400. loss=Array(2.179274, dtype=float32), eval_loss=Array(2.719959, dtype=float32)\n",
      "4500. loss=Array(1.0936526, dtype=float32), eval_loss=Array(2.6545784, dtype=float32)\n",
      "4600. loss=Array(1.6174341, dtype=float32), eval_loss=Array(2.637119, dtype=float32)\n",
      "4700. loss=Array(2.3421094, dtype=float32), eval_loss=Array(2.6615534, dtype=float32)\n",
      "4800. loss=Array(3.0838943, dtype=float32), eval_loss=Array(2.694758, dtype=float32)\n",
      "4900. loss=Array(3.0270154, dtype=float32), eval_loss=Array(2.670301, dtype=float32)\n",
      "5000. loss=Array(1.9192, dtype=float32), eval_loss=Array(2.654093, dtype=float32)\n",
      "5100. loss=Array(2.7786174, dtype=float32), eval_loss=Array(2.6606884, dtype=float32)\n",
      "5200. loss=Array(2.4341536, dtype=float32), eval_loss=Array(2.6726716, dtype=float32)\n",
      "5300. loss=Array(2.8916962, dtype=float32), eval_loss=Array(2.6524403, dtype=float32)\n",
      "5400. loss=Array(2.1269367, dtype=float32), eval_loss=Array(2.6491542, dtype=float32)\n",
      "5500. loss=Array(2.3413222, dtype=float32), eval_loss=Array(2.6695874, dtype=float32)\n",
      "5600. loss=Array(1.2567987, dtype=float32), eval_loss=Array(2.6447453, dtype=float32)\n",
      "5700. loss=Array(1.8756931, dtype=float32), eval_loss=Array(2.6307497, dtype=float32)\n",
      "5800. loss=Array(1.9631459, dtype=float32), eval_loss=Array(2.6670825, dtype=float32)\n",
      "5900. loss=Array(2.1714895, dtype=float32), eval_loss=Array(2.6287963, dtype=float32)\n",
      "6000. loss=Array(2.0833068, dtype=float32), eval_loss=Array(2.6334856, dtype=float32)\n",
      "6100. loss=Array(2.592342, dtype=float32), eval_loss=Array(2.6196787, dtype=float32)\n",
      "6200. loss=Array(2.8794312, dtype=float32), eval_loss=Array(2.642631, dtype=float32)\n",
      "6300. loss=Array(1.7011226, dtype=float32), eval_loss=Array(2.6091475, dtype=float32)\n",
      "6400. loss=Array(2.2800558, dtype=float32), eval_loss=Array(2.628773, dtype=float32)\n",
      "6500. loss=Array(3.061575, dtype=float32), eval_loss=Array(2.6371639, dtype=float32)\n",
      "6600. loss=Array(1.7783184, dtype=float32), eval_loss=Array(2.6433327, dtype=float32)\n",
      "6700. loss=Array(2.0598748, dtype=float32), eval_loss=Array(2.6300628, dtype=float32)\n",
      "6800. loss=Array(1.1033232, dtype=float32), eval_loss=Array(2.657799, dtype=float32)\n",
      "6900. loss=Array(2.9768546, dtype=float32), eval_loss=Array(2.649463, dtype=float32)\n",
      "7000. loss=Array(2.9457445, dtype=float32), eval_loss=Array(2.6465805, dtype=float32)\n",
      "7100. loss=Array(2.07439, dtype=float32), eval_loss=Array(2.6103356, dtype=float32)\n",
      "7200. loss=Array(2.365795, dtype=float32), eval_loss=Array(2.5841887, dtype=float32)\n",
      "7300. loss=Array(2.9250963, dtype=float32), eval_loss=Array(2.591701, dtype=float32)\n",
      "7400. loss=Array(2.4413433, dtype=float32), eval_loss=Array(2.6260846, dtype=float32)\n",
      "7500. loss=Array(2.1212018, dtype=float32), eval_loss=Array(2.587921, dtype=float32)\n",
      "7600. loss=Array(2.4497557, dtype=float32), eval_loss=Array(2.591892, dtype=float32)\n",
      "7700. loss=Array(1.6514273, dtype=float32), eval_loss=Array(2.6315868, dtype=float32)\n",
      "7800. loss=Array(1.3464855, dtype=float32), eval_loss=Array(2.6301024, dtype=float32)\n",
      "7900. loss=Array(2.5935276, dtype=float32), eval_loss=Array(2.625608, dtype=float32)\n",
      "8000. loss=Array(2.155299, dtype=float32), eval_loss=Array(2.6216078, dtype=float32)\n",
      "8100. loss=Array(2.245656, dtype=float32), eval_loss=Array(2.6305034, dtype=float32)\n",
      "8200. loss=Array(1.9246649, dtype=float32), eval_loss=Array(2.6319506, dtype=float32)\n",
      "8300. loss=Array(2.168799, dtype=float32), eval_loss=Array(2.581287, dtype=float32)\n",
      "8400. loss=Array(1.3653765, dtype=float32), eval_loss=Array(2.579614, dtype=float32)\n",
      "8500. loss=Array(2.949377, dtype=float32), eval_loss=Array(2.5678747, dtype=float32)\n",
      "8600. loss=Array(2.1316032, dtype=float32), eval_loss=Array(2.5586503, dtype=float32)\n",
      "8700. loss=Array(2.4669316, dtype=float32), eval_loss=Array(2.5652282, dtype=float32)\n",
      "8800. loss=Array(3.0046916, dtype=float32), eval_loss=Array(2.5545897, dtype=float32)\n",
      "8900. loss=Array(2.2027574, dtype=float32), eval_loss=Array(2.5792933, dtype=float32)\n",
      "9000. loss=Array(1.7540431, dtype=float32), eval_loss=Array(2.5881257, dtype=float32)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/arturgalstyan/Workspace/learning-the-transformer/transformer.ipynb Cell 12\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/arturgalstyan/Workspace/learning-the-transformer/transformer.ipynb#X14sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m     transformer, opt_state, loss \u001b[39m=\u001b[39m step(transformer, opt_state, optimiser, x, y)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/arturgalstyan/Workspace/learning-the-transformer/transformer.ipynb#X14sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m     \u001b[39mif\u001b[39;00m i \u001b[39m%\u001b[39m \u001b[39m100\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/arturgalstyan/Workspace/learning-the-transformer/transformer.ipynb#X14sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m         eval_loss \u001b[39m=\u001b[39m evaluate(transformer, test_dataloader)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/arturgalstyan/Workspace/learning-the-transformer/transformer.ipynb#X14sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m=}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{\u001b[39;00meval_loss\u001b[39m=}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/arturgalstyan/Workspace/learning-the-transformer/transformer.ipynb#X14sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mdone.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m/Users/arturgalstyan/Workspace/learning-the-transformer/transformer.ipynb Cell 12\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/arturgalstyan/Workspace/learning-the-transformer/transformer.ipynb#X14sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     x \u001b[39m=\u001b[39m jnp\u001b[39m.\u001b[39marray(x\u001b[39m.\u001b[39mnumpy())\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/arturgalstyan/Workspace/learning-the-transformer/transformer.ipynb#X14sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     y \u001b[39m=\u001b[39m jnp\u001b[39m.\u001b[39marray(y\u001b[39m.\u001b[39mnumpy())\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/arturgalstyan/Workspace/learning-the-transformer/transformer.ipynb#X14sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m jitted_loss_fn(transformer, x, y)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/arturgalstyan/Workspace/learning-the-transformer/transformer.ipynb#X14sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mreturn\u001b[39;00m loss \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(test_dataloader)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.12/lib/python3.10/site-packages/equinox/_jit.py:107\u001b[0m, in \u001b[0;36m_JitWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m/\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 107\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39mFalse\u001b[39;49;00m, args, kwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.12/lib/python3.10/site-packages/equinox/_jit.py:85\u001b[0m, in \u001b[0;36m_JitWrapper._call\u001b[0;34m(self, is_lower, args, kwargs)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call\u001b[39m(\u001b[39mself\u001b[39m, is_lower, args, kwargs):\n\u001b[1;32m     84\u001b[0m     args, kwargs \u001b[39m=\u001b[39m _bind(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_signature, args, kwargs)\n\u001b[0;32m---> 85\u001b[0m     dynamic_spec, static_spec \u001b[39m=\u001b[39m hashable_partition((args, kwargs), is_array)\n\u001b[1;32m     86\u001b[0m     dynamic \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dynamic_fun, dynamic_spec)\n\u001b[1;32m     87\u001b[0m     static \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_static_fun, static_spec)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.12/lib/python3.10/site-packages/equinox/_compile_utils.py:22\u001b[0m, in \u001b[0;36mhashable_partition\u001b[0;34m(pytree, filter_fn)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mhashable_partition\u001b[39m(pytree: PyTree, filter_fn: Callable):\n\u001b[0;32m---> 22\u001b[0m     leaves, treedef \u001b[39m=\u001b[39m jtu\u001b[39m.\u001b[39;49mtree_flatten(pytree)\n\u001b[1;32m     23\u001b[0m     dynamic_leaves \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(x \u001b[39mif\u001b[39;00m filter_fn(x) \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m leaves)\n\u001b[1;32m     24\u001b[0m     static_leaves \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(\u001b[39mNone\u001b[39;00m \u001b[39mif\u001b[39;00m filter_fn(x) \u001b[39melse\u001b[39;00m x \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m leaves)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.12/lib/python3.10/site-packages/jax/_src/tree_util.py:76\u001b[0m, in \u001b[0;36mtree_flatten\u001b[0;34m(tree, is_leaf)\u001b[0m\n\u001b[1;32m     74\u001b[0m   \u001b[39mreturn\u001b[39;00m default_registry\u001b[39m.\u001b[39mflatten(tree, is_leaf)\n\u001b[1;32m     75\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 76\u001b[0m   \u001b[39mreturn\u001b[39;00m pytree\u001b[39m.\u001b[39;49mflatten(tree, is_leaf)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.12/lib/python3.10/site-packages/equinox/_module.py:297\u001b[0m, in \u001b[0;36m_flatten_module\u001b[0;34m(module, with_keys)\u001b[0m\n\u001b[1;32m    295\u001b[0m         wrapper_field_names\u001b[39m.\u001b[39mappend(name)\n\u001b[1;32m    296\u001b[0m         wrapper_field_values\u001b[39m.\u001b[39mappend(value)\n\u001b[0;32m--> 297\u001b[0m aux \u001b[39m=\u001b[39m _FlattenedData(\n\u001b[1;32m    298\u001b[0m     \u001b[39mtuple\u001b[39;49m(dynamic_field_names),\n\u001b[1;32m    299\u001b[0m     \u001b[39mtuple\u001b[39;49m(static_field_names),\n\u001b[1;32m    300\u001b[0m     \u001b[39mtuple\u001b[39;49m(static_field_values),\n\u001b[1;32m    301\u001b[0m     \u001b[39mtuple\u001b[39;49m(wrapper_field_names),\n\u001b[1;32m    302\u001b[0m     \u001b[39mtuple\u001b[39;49m(wrapper_field_values),\n\u001b[1;32m    303\u001b[0m )\n\u001b[1;32m    304\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mtuple\u001b[39m(dynamic_field_values), aux\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tinyshakespeareloader.hamlet import get_data\n",
    "\n",
    "\n",
    "data = get_data()\n",
    "\n",
    "\n",
    "train_dataloader, test_dataloader, vocabulary_size, chars, encode, decode = data[\"train_dataloader\"], data[\"test_dataloader\"], data[\"vocabulary_size\"], data[\"chars\"], data[\"encode\"], data[\"decode\"]\n",
    "key = jax.random.PRNGKey(420)\n",
    "INPUT_DIMS: int = int(vocabulary_size)\n",
    "N_EMBD = 32\n",
    "N_HEADS = 4\n",
    "MAX_T = 8\n",
    "\n",
    "def loss_fn(transformer: Transformer, x: Array, y: Array):\n",
    "    logits = eqx.filter_vmap(transformer)(x)\n",
    "    loss = optax.softmax_cross_entropy_with_integer_labels(logits, y)\n",
    "\n",
    "    return jnp.mean(loss)\n",
    "\n",
    "def evaluate(transformer: Transformer, test_dataloader):\n",
    "    loss = 0\n",
    "    jitted_loss_fn = eqx.filter_jit(loss_fn)\n",
    "    for x, y in test_dataloader:\n",
    "        x = jnp.array(x.numpy())\n",
    "        y = jnp.array(y.numpy())\n",
    "        loss += jitted_loss_fn(transformer, x, y)\n",
    "    \n",
    "    return loss / len(test_dataloader)\n",
    "\n",
    "@eqx.filter_jit\n",
    "def step(transformer: PyTree, opt_state: optax.OptState, optimiser: optax.GradientTransformation, x: Array, y: Array):\n",
    "    loss, grads = eqx.filter_value_and_grad(loss_fn)(transformer, x, y)\n",
    "    updates, opt_state = optimiser.update(grads, opt_state, transformer)\n",
    "    transformer = eqx.apply_updates(transformer, updates)\n",
    "    return transformer, opt_state, loss\n",
    "\n",
    "transformer = Transformer(n_dims=INPUT_DIMS, n_embd=N_EMBD, n_heads=N_HEADS, key=key)\n",
    "#start_loss = evaluate(transformer, test_dataloader)\n",
    "#print(f\"{start_loss=}\")\n",
    "optimiser = optax.adamw(learning_rate=0.001)\n",
    "opt_state = optimiser.init(eqx.filter(transformer, eqx.is_inexact_array))\n",
    "for i, (x, y) in enumerate(train_dataloader):\n",
    "    x = jnp.array(x.numpy())\n",
    "    y = jnp.array(y.numpy())\n",
    "    transformer, opt_state, loss = step(transformer, opt_state, optimiser, x, y)\n",
    "    if i % 100 == 0:\n",
    "        eval_loss = evaluate(transformer, test_dataloader)\n",
    "        print(f\"{i}. {loss=}, {eval_loss=}\")\n",
    "\n",
    "print(\"done.\")\n",
    "print(f\"{evaluate(transformer, test_dataloader)=}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
