{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multihead-Attention\n",
    "Multihead-Attention (MHA) is the key part of the transformer architecture. It uses a mechanism called _self attention_, which has been very successful in NLP tasks. I already introduced the transformer in my other [blog post](/posts/learning-the-transformer). In this blog post, we will take a deepdive into the MHA and try to make it as general as we can. Let's start with an overview of the MHA block. In the following figure, you can see the structure of the MHA block. \n",
    "\n",
    "![MHA Overview](MHA.drawio.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input to the MHA block is duplicated into 3 vectors: the _query_, _key_ and _value_ vectors. Each of those is first passed through their respective linear layer. Each of those layers has a specific task:\n",
    "\n",
    "- Query Layer: transforms the input to query vectors, i.e. _what you're interested in_\n",
    "- Key Layer: transforms the input to a set of keys to match the query vectors against\n",
    "- Value Layer: take the scaled combination of the query and key projects and compute the output of the MHA block\n",
    "\n",
    "This is a good starting point, so let's write this down in code.\n",
    "\n",
    "_(By the way, most of this was already covered in my [previous blog post](/posts/learning-the-transformer) and this implementation takes heavy inspiration from already existing implementations such as the [MHA block from Equinox](https://github.com/patrick-kidger/equinox/blob/main/equinox/nn/_attention.py))_\n",
    "\n",
    "One thing to note is the dimensionalities of the vectors, so let's start by defining the dimensions first. Here's the notation for this blog post:\n",
    "\n",
    "- $L$: maximum sequence length\n",
    "- $h$: number of heads\n",
    "- $\\{q,k,v\\}_{emdb}$: query, key or value embedding dimension\n",
    "\n",
    "Furthermore, let's define the input to the MHA block.\n",
    "\n",
    "- Query: $[L \\times q_{in}]$, where $q_{in}$ is the query input dimension\n",
    "- Key: $[L \\times k_{in}]$, where $k_{in}$ is the key input dimension\n",
    "- Value $[L \\times v_{in}]$, where $v_{in}$ is the value input dimension\n",
    "\n",
    "Usually, the query, key and value input dimensions are the same, but we want our implementation to be very general and make as few assumptions as possible about the current use case. Therefore, we will be more specific. The reason that normally they are the same is that, typically, they come out of the input embeddings (with the positional embeddings added on top) and the same embedding is used for all vectors, giving them all the same input dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import equinox as eqx\n",
    "from jaxtyping import Float, Array\n",
    "\n",
    "\n",
    "query_input_dim = 16\n",
    "query_embedding_dim = 32\n",
    "key_input_dim = 16\n",
    "key_embedding_dim = 32\n",
    "value_input_dim = 16\n",
    "value_embedding_dim = 32\n",
    "num_heads = 4\n",
    "max_seq_len = 10\n",
    "batch_size = 2\n",
    "key = jax.random.PRNGKey(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query.shape=(10, 4, 32)\n",
      "key.shape=(10, 4, 32)\n",
      "value.shape=(10, 4, 32)\n"
     ]
    }
   ],
   "source": [
    "# Version 1\n",
    "class MultiheadAttention(eqx.Module):\n",
    "    query_projection: eqx.nn.Linear\n",
    "    key_projection: eqx.nn.Linear\n",
    "    value_projection: eqx.nn.Linear\n",
    "\n",
    "    query_input_dim: int = eqx.field(static=True)\n",
    "    query_embedding_dim: int = eqx.field(static=True)\n",
    "    \n",
    "    key_input_dim: int = eqx.field(static=True)\n",
    "    key_embedding_dim: int = eqx.field(static=True)\n",
    "\n",
    "    value_input_dim: int = eqx.field(static=True)\n",
    "    value_embedding_dim: int = eqx.field(static=True)\n",
    "\n",
    "    num_heads: int = eqx.field(static=True)\n",
    "\n",
    "    def __init__(self, query_embedding_dim, key_embedding_dim, value_embedding_dim, query_input_dim, key_input_dim, value_input_dim, num_heads, key):\n",
    "        qkey, kkey, vkey = jax.random.split(key, 3)\n",
    "        self.query_projection = eqx.nn.Linear(query_input_dim, num_heads * query_embedding_dim, key=qkey, use_bias=False)\n",
    "        self.key_projection = eqx.nn.Linear(key_input_dim, num_heads * key_embedding_dim, key=kkey, use_bias=False)\n",
    "        self.value_projection = eqx.nn.Linear(value_input_dim, num_heads * value_embedding_dim, key=vkey, use_bias=False)\n",
    "    \n",
    "        # parameters\n",
    "        self.query_input_dim = query_input_dim\n",
    "        self.query_embedding_dim = query_embedding_dim\n",
    "        self.key_input_dim = key_input_dim\n",
    "        self.key_embedding_dim = key_embedding_dim\n",
    "        self.value_input_dim = value_input_dim\n",
    "        self.value_embedding_dim = value_embedding_dim\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "    def __call__(self, x: Float[Array, \"max_seq_len input_dim\"]):\n",
    "        seq_len, _ = x.shape\n",
    "        query = jax.vmap(self.query_projection)(x).reshape(seq_len, self.num_heads, self.query_embedding_dim)\n",
    "        key = jax.vmap(self.key_projection)(x).reshape(seq_len, self.num_heads, self.key_embedding_dim) \n",
    "        value = jax.vmap(self.value_projection)(x).reshape(seq_len, self.num_heads, self.value_embedding_dim)\n",
    "        print(f\"{query.shape=}\")\n",
    "        print(f\"{key.shape=}\")\n",
    "        print(f\"{value.shape=}\")\n",
    "\n",
    "key, subkey = jax.random.split(key)\n",
    "mha = MultiheadAttention(query_embedding_dim, key_embedding_dim, value_embedding_dim, query_input_dim, key_input_dim, value_input_dim, num_heads, key)\n",
    "x = jax.random.normal(subkey, (max_seq_len, query_input_dim))\n",
    "mha(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned in my previous blog post, a MHA block consists of multiple _heads_. But, instead of looping over each head, one at a time, we can instead simply enlarge the query, key and value layers to include all of the heads. Look at it this way: taking all of the heads into consideration, the **output** shape of, say, the query projection should be:\n",
    "\n",
    "$$\n",
    "    [L \\times h \\times q_{embd}]\n",
    "$$\n",
    "\n",
    "By making the query projection layer project from $q_{in}$ to $h * q_{emdb}$, we get initially a matrix with the same $[L \\times h * q_{embd}]$. From there, we can simply reshape that matrix into our desired shape: $[L \\times h \\times q_{embd}]$.\n",
    "\n",
    "This is just the first steps for the query and value projections, they have still quite the journey ahead. We still need to matrix multiply, scale (sometimes mask) and softmax them. Let's write a function that can do all of that in one go. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
